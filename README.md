# ACME Supermarkets Data Pipeline

A comprehensive Databricks Delta Live Tables (DLT) pipeline project built with [LakehousePlumber](https://github.com/mmodarre/lakehouse_plumber), implementing a modern medallion architecture for retail data analytics.

## ðŸ“‹ Overview

This project implements an enterprise-grade data pipeline for ACME Supermarkets, integrating data from multiple source systems:

- **NCR** - Point of Sale (POS) transactions and terminal data
- **SAP** - Enterprise Resource Planning (ERP) data including inventory, purchasing, and warehouse operations
- **SFCC** (Salesforce Commerce Cloud) - E-commerce platform data including online orders and customer information

The pipeline follows the **medallion architecture** pattern (Raw â†’ Bronze â†’ Silver) to progressively refine data quality and structure:

1. **Raw Layer** - Ingests raw data files from landing zones with minimal transformation
2. **Bronze Layer** - Cleanses and standardizes data, adds operational metadata
3. **Silver Layer** - Creates analytics-ready dimensions and facts with:
   - **Dimensions** (SCD Type 2) - Customer, product, location, supplier, etc.
   - **Facts** - Transactional data including POS transactions, sales orders, inventory movements, etc.

## ðŸ—ï¸ Project Architecture

```
acme_supermarkets_lhp/
â”œâ”€â”€ pipelines/              # Pipeline configurations by layer
â”‚   â”œâ”€â”€ 01_raw_ingestion/   # Raw data ingestion configs
â”‚   â”‚   â”œâ”€â”€ NCR/            # 7 tables: POS, terminals, locations, etc.
â”‚   â”‚   â”œâ”€â”€ SAP/            # 17 tables: ERP, inventory, purchasing, etc.
â”‚   â”‚   â””â”€â”€ SFCC/           # 7 tables: E-commerce orders, customers, etc.
â”‚   â”œâ”€â”€ 02_bronze/          # Bronze layer transformations
â”‚   â”‚   â”œâ”€â”€ NCR/, SAP/, SFCC/
â”‚   â””â”€â”€ 03_silver/          # Analytics-ready data
â”‚       â”œâ”€â”€ dimensions/     # 15 dimension tables (SCD2)
â”‚       â””â”€â”€ facts/          # 16 fact tables
â”‚
â”œâ”€â”€ templates/              # Reusable LHP templates
â”‚   â”œâ”€â”€ TMPL001_csv_ingestion_template.yaml
â”‚   â”œâ”€â”€ TMPL002_json_text_ingestion_template.yaml
â”‚   â”œâ”€â”€ TMPL003_parquet_ingestion_template.yaml
â”‚   â”œâ”€â”€ TMPL004_raw_to_bronze_standard_template.yaml
â”‚   â”œâ”€â”€ TMPL005_silver_dimension_scd2_template.yaml
â”‚   â”œâ”€â”€ TMPL006_silver_fact_scd2_template.yaml
â”‚   â”œâ”€â”€ TMPL007_silver_accumulating_fact_template.yaml
â”‚   â”œâ”€â”€ TMPL008_silver_snapshot_dimension_scd2_template.yaml
â”‚   â””â”€â”€ TMPL009_silver_transactional_fact_template.yaml
â”‚
â”œâ”€â”€ schemas/                # Table schema definitions
â”‚   â”œâ”€â”€ customer_schema.yaml, product.yaml, etc.
â”‚   â””â”€â”€ (29 schema files covering all entities)
â”‚
â”œâ”€â”€ substitutions/          # Environment-specific configurations
â”‚   â”œâ”€â”€ dev.yaml            # Development environment
â”‚   â”œâ”€â”€ tst.yaml            # Test environment
â”‚   â””â”€â”€ prd.yaml            # Production environment
â”‚
â”œâ”€â”€ expectations/           # Data quality rules
â”‚   â”œâ”€â”€ check_no_rescued_data.json
â”‚   â””â”€â”€ customer_quality.json.tmpl
â”‚
â”œâ”€â”€ resources/              # Databricks resources
â”‚   â”œâ”€â”€ lhp/                # Generated DLT pipeline YAML files
â”‚   â””â”€â”€ acme_supermarkets_lhp_orchestration.job.yml
â”‚
â”œâ”€â”€ py_functions/           # Custom Python functions
â”‚   â”œâ”€â”€ product_snapshot_func.py
â”‚   â””â”€â”€ timestamp_converter.py
â”‚
â”œâ”€â”€ generated/              # Generated DLT Python code (do not edit)
â”‚   â””â”€â”€ dev/                # Environment-specific generated code
â”‚
â”œâ”€â”€ docs/                   # Documentation
â”‚   â””â”€â”€ Database_schema.md  # Complete database schema reference
â”‚
â”œâ”€â”€ lhp.yaml                # LakehousePlumber project config
â””â”€â”€ databricks.yml          # Databricks Asset Bundles config
```

## ðŸ“Š Data Model

The project implements a comprehensive retail data model covering:

- **Reference Data**: Brands, categories, payment methods, carriers, users
- **ERP**: Locations, products, suppliers, purchase orders, goods receipts, transfers
- **CRM**: Customers, addresses, payment methods
- **Inventory**: Store and warehouse inventory tracking with transaction history
- **POS**: Point of sale transactions, payments, terminals, promotions
- **E-commerce**: Online orders, shipments, fulfillment tracking
- **Loyalty**: Loyalty programs, accounts, and transactions
- **Financial**: Chart of accounts, journal entries

See `docs/Database_schema.md` for complete schema documentation.

## ðŸš€ Getting Started

### Prerequisites

- **Python 3.8+**
- **LakehousePlumber** (>= 0.5.0, < 0.7.0)
- **Databricks workspace** with Delta Live Tables enabled
- **Databricks CLI** configured

### Installation

1. **Install LakehousePlumber**:
   ```bash
   pip install lakehouse-plumber
   ```

2. **Clone this repository**:
   ```bash
   git clone <repository-url>
   cd acme_supermarkets_lhp
   ```

3. **Configure your environment**:
   - Edit `substitutions/dev.yaml` with your environment-specific values (catalogs, schemas, paths)
   - Update `databricks.yml` with your workspace details

### Quick Start

1. **Validate all pipeline configurations**:
   ```bash
   lhp validate --env dev
   ```

2. **Generate DLT pipeline code**:
   ```bash
   lhp generate --env dev
   ```

3. **Deploy to Databricks** (using Databricks Asset Bundles):
   ```bash
   databricks bundle deploy -t dev
   ```

4. **Run the orchestration job**:
   ```bash
   databricks bundle run acme_supermarkets_lhp_orchestration -t dev
   ```

## ðŸ”§ Development Workflow

### Creating a New Pipeline

1. **Create a pipeline configuration directory**:
   ```bash
   mkdir -p pipelines/04_gold/aggregates
   ```

2. **Create a flowgroup YAML file**:
   ```bash
   touch pipelines/04_gold/aggregates/sales_summary.yaml
   ```

3. **Define your pipeline using templates**:
   ```yaml
   pipeline: gold_aggregates
   flowgroup: sales_summary
   
   use_template: materialized_view_template
   template_parameters:
     table_name: daily_sales_summary
     source_tables: 
       - fact_pos_transaction
       - dim_product
   ```

4. **Validate the configuration**:
   ```bash
   lhp validate --env dev
   ```

5. **Generate and preview** (dry-run mode):
   ```bash
   lhp generate --env dev --dry-run --verbose
   ```

6. **Generate the actual code**:
   ```bash
   lhp generate --env dev
   ```

### Working with Templates

**List available templates**:
```bash
lhp list-templates
```

**Show template details**:
```bash
lhp show-template TMPL005_silver_dimension_scd2_template
```

**Validate templates only**:
```bash
lhp validate --env dev --templates-only
```

### Working with Presets

**List available presets**:
```bash
lhp list-presets
```

**Show preset configuration**:
```bash
lhp show-preset bronze_layer
```

## ðŸ“š Available Commands

| Command | Description |
|---------|-------------|
| `lhp validate --env <env>` | Validate all pipeline configurations for specified environment |
| `lhp generate --env <env>` | Generate DLT Python code from pipeline configurations |
| `lhp list-presets` | List all available reusable presets |
| `lhp list-templates` | List all available templates |
| `lhp show <flowgroup>` | Show resolved configuration for a specific flowgroup |
| `lhp show-template <name>` | Display template details |
| `lhp --version` | Show LakehousePlumber version |

### Additional Flags

- `--dry-run` - Preview changes without writing files
- `--verbose` - Enable detailed logging
- `--templates-only` - Validate/operate on templates only

## ðŸŒ Environments

The project supports multiple environments with environment-specific configurations in `substitutions/`:

- **dev** - Development environment for testing and iteration
- **tst** - Test environment for validation
- **prd** - Production environment

Each environment can have different:
- Catalog and schema names
- Storage paths and volumes
- Secret scopes and credentials
- API endpoints

## ðŸŽ¯ Data Quality

Data quality expectations are defined in the `expectations/` directory:

- **check_no_rescued_data.json** - Ensures no data is rescued during ingestion
- **customer_quality.json.tmpl** - Customer data quality rules

Expectations are automatically applied during DLT pipeline execution.

## ðŸ“– Project Metadata

- **Name**: acme_supermarkets_data_pipeline
- **Version**: 1.0
- **Author**: Mehdi Modarressi
- **Created**: 2025-10-15
- **Required LHP Version**: >= 0.5.0, < 0.7.0

## ðŸ”— Additional Resources

- [LakehousePlumber Documentation](https://github.com/mmodarre/lakehouse_plumber)
- [Databricks Delta Live Tables](https://docs.databricks.com/delta-live-tables/index.html)
- [Database Schema Documentation](docs/Database_schema.md)
- [Databricks Asset Bundles](https://docs.databricks.com/dev-tools/bundles/index.html)

## ðŸ¤ Contributing

When contributing to this project:

1. Always validate configurations before committing: `lhp validate --env dev`
2. Never edit files in the `generated/` directory directly
3. Use templates for consistency and maintainability
4. Update schema files when data models change
5. Test changes in `dev` environment before promoting

## ðŸ“ Notes

- **Generated code** in `generated/` directory is auto-generated - do not edit manually
- **Operational metadata** columns (`_source_file_path`, `_processing_timestamp`, `_source_file_name`) are automatically added to tables
- **Version control**: Only commit source configurations (pipelines, templates, schemas), not generated code
- **Templates vs Pipelines**: Templates are reusable patterns, pipelines are specific implementations

---

*Built with â¤ï¸ using [LakehousePlumber](https://github.com/mmodarre/lakehouse_plumber)* 